{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wLl-QZ_-ACGN",
        "outputId": "ab47aeba-b1f5-4fee-928b-6e5cc49cf71e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.18.0-py3-none-any.whl (510 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m510.5/510.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets) (3.13.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.25.2)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.66.2)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.4.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.16-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.8/134.8 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]<=2024.2.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.19.4 in /usr/local/lib/python3.10/dist-packages (from datasets) (0.20.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.19.4->datasets) (4.10.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2023.4)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, datasets\n",
            "Successfully installed datasets-2.18.0 dill-0.3.8 multiprocess-0.70.16 xxhash-3.4.1\n",
            "Requirement already satisfied: transformers[torch] in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (3.13.3)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.20.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (4.66.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (2.2.1+cu121)\n",
            "Requirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from transformers[torch]) (0.28.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.21.0->transformers[torch]) (5.9.5)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers[torch]) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (3.1.3)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch->transformers[torch]) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->transformers[torch]) (12.4.99)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers[torch]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->transformers[torch]) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->transformers[torch]) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install datasets\n",
        "!pip install accelerate>=0.21.0\n",
        "!pip install transformers[torch]\n",
        "#!pip3 install evaluate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uH6u3wG7ho7Z"
      },
      "outputs": [],
      "source": [
        "\n",
        "!pip3 install pandas\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vXxP37Nsf6os",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 304,
          "referenced_widgets": [
            "6749165758614836bf145f18ab612d52",
            "9fce8c5596c14f2f8ece65a7c430f6ff",
            "3b2059a8fc2348708b8f2268de981596",
            "8f333fc0bff34bcc8b1648a22c373bf0",
            "242a1be3a5ff4e1390e98b54f567827a",
            "03ef4d9cddba4c22a1398525465895b8",
            "14bead6ae4904c7eaf75ada1297f5a90",
            "d78699cbba7d43b696e9cca8e84d00ef",
            "3a13601d3d344f12b751e0ccc8e3ef85",
            "e237392c09104efa8ca4150d9f36f972",
            "a9cb9efd8aa14b84a4c33a82eff4e7dc",
            "143ab905681c4e0c993985885a748391",
            "38f87f5b7966481db28c410adf780ec6",
            "53bba74dad20454396c6d5ee5fbed06f",
            "87cd41e8995245c9873e7879a4b353a7",
            "afe78fca3d92478db1ef2b6b6798d2d1",
            "ab0091b4dff74b4595c5c0448188b8b0",
            "0b86a28da9a1407397fc060f2486495a",
            "3eacc944cf2645adb1e1db0993ea6294",
            "8fbe8b5f2de540b18b0e89a0bf66b9b6",
            "7f326ff878ed449fabd3dae56cfe3eee",
            "0896619ec4034ff88b4d8bc4ccdbd117",
            "9589e7c3dc274a38b437bb52cead0842",
            "ed0bf3a5d1694735bd2a0b5620b7fbb3",
            "11ac72cf9e32441e9d30877b3a6ee5a3",
            "5c170b7245f743e0bfda9b542832006f",
            "39aaa7267f5f49d7bc53d9a27eeb3585",
            "2800fb313789421799729b6f5bb7de00",
            "7de623ccda65466c93ddd3133bf0a68d",
            "594b211ac9934c46bc6733ba30749afa",
            "79e00c2dea6c469890bce0b19ed8560b",
            "81d1f1bb729c4a4fb1ee15e470417029",
            "f9e1959105d04373b748a46807e4358c",
            "135430c6a5d04147ab6bc45f7b8380b4",
            "cfc487246dbf4bfeaac046ddec2d51d3",
            "cbc92df1f95949a49d4343193c2ce253",
            "54bdcf8101e64a8ab604050bce071242",
            "8f160f2b775b43e6be4d42989e195afa",
            "75cd40989fbc4ca38d5e7ceef7f0ad04",
            "c126076c240c41fc9837b6c256b348de",
            "b5309dddfbe64eeb9166c969d009887b",
            "f978e8b9eb894138bc8f5f46d1de7c4c",
            "d9f1a73e50994e4ab40f200a9df84b8a",
            "ad2726c74f2f4de989710040fc81a3b7"
          ]
        },
        "outputId": "fdede811-a0b7-4882-a3a4-519912f7ef02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:88: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/78.3k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6749165758614836bf145f18ab612d52"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading data:   0%|          | 0.00/6.27k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "143ab905681c4e0c993985885a748391"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating train split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9589e7c3dc274a38b437bb52cead0842"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Generating test split: 0 examples [00:00, ? examples/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "135430c6a5d04147ab6bc45f7b8380b4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                data\n",
            "0  [{'paragraphs': [{'context': 'Digital Image Pr...\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"finalgp3/dataset\")\n",
        "df = pd.DataFrame(dataset['train'])\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43Fa1d63DpsP",
        "outputId": "df9b2278-abe0-4471-d29c-40abf50ef424"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DatasetDict({\n",
            "    train: Dataset({\n",
            "        features: ['data'],\n",
            "        num_rows: 1\n",
            "    })\n",
            "    test: Dataset({\n",
            "        features: ['data'],\n",
            "        num_rows: 1\n",
            "    })\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VLsYDS7QhYmU"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "def tokenize_and_format(examples):\n",
        "    tokenized_inputs = tokenizer(examples[\"content\"], padding=\"max_length\", truncation=True, max_length=1000)  # Reduced max_length\n",
        "    tokenized_inputs[\"labels\"] = tokenized_inputs[\"input_ids\"].copy()\n",
        "    return tokenized_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(tokenize_and_format, batched=True)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Assuming dataset is already loaded using load_dataset\n",
        "def inspect_dataset(dataset_split):\n",
        "    for i, item in enumerate(dataset_split):\n",
        "        print(f\"Item {i}: {item}\")\n",
        "        if i >= 2:  # Adjust this number to inspect more items if needed\n",
        "            break\n",
        "\n",
        "print(\"Inspecting Train Dataset:\")\n",
        "inspect_dataset(dataset['train'])\n",
        "\n",
        "print(\"\\nInspecting Test Dataset:\")\n",
        "inspect_dataset(dataset['test'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_fekXITqbTa",
        "outputId": "f4ac9bb3-8869-4044-c70c-a23b049d7419"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Inspecting Train Dataset:\n",
            "Item 0: {'data': [{'paragraphs': [{'context': 'Digital Image Processing Lecture 4: Filters (Part 2) & Edges and Contours Dr. Belhassen Akrout Computer Science Dept. What is anEdge? Edge? sharp change in brightness (discontinuities).', 'qas': [{'question': 'What is the main focus of Lecture 4 in Digital Image Processing?', 'id': 'unique_question_identifier_001', 'answers': [{'text': 'Filters (Part 2) & Edges and Contours', 'answer_start': 30}]}, {'question': 'Who is delivering Lecture 4 on Digital Image Processing?', 'id': 'unique_question_identifier_002', 'answers': [{'text': 'Dr. Belhassen Akrout', 'answer_start': 87}]}]}, {'context': 'Where do edges occur? Actual edges: Boundaries between objects Sharp change in brightness can also occur within object Reflectance changes Change in surface orientation Illumination changes.', 'qas': [{'question': 'Where do edges typically occur in image processing?', 'id': 'unique_question_identifier_003', 'answers': [{'text': 'Boundaries between objects', 'answer_start': 34}]}, {'question': 'What can cause a sharp change in brightness besides edges?', 'id': 'unique_question_identifier_004', 'answers': [{'text': 'Reflectance changes, change in surface orientation, illumination changes', 'answer_start': 115}]}]}, {'context': 'Edge Detection Image processing task that finds edges and contours in images. Edges so important that human vision can reconstruct edge lines.', 'qas': [{'question': 'What is the purpose of edge detection in image processing?', 'id': 'unique_question_identifier_005', 'answers': [{'text': 'Finds edges and contours in images', 'answer_start': 29}]}, {'question': 'Why are edges important in human vision?', 'id': 'unique_question_identifier_006', 'answers': [{'text': 'Human vision can reconstruct edge lines', 'answer_start': 77}]}]}, {'context': 'Characteristics of an Edge: Edge is a sharp change in brightness. Ideal edge is a step function in some direction. Real (non ideal) edge is a slightly blurred step function. Edges can be characterized by high value first derivative.', 'qas': [{'question': 'How is an ideal edge characterized in image processing?', 'id': 'unique_question_identifier_007', 'answers': [{'text': 'A step function in some direction', 'answer_start': 74}]}, {'question': 'How does a real edge differ from an ideal edge?', 'id': 'unique_question_identifier_008', 'answers': [{'text': 'Real edge is a slightly blurred step function', 'answer_start': 108}]}]}, {'context': 'Rising slope causes positive + high value first derivative. Falling slope causes negative + high value first derivative. Ideal edge is a step function in certain direction. First derivative of I(x) has a peak at the edge.', 'qas': [{'question': 'What does a rising slope in an edge indicate?', 'id': 'unique_question_identifier_009', 'answers': [{'text': 'Positive + high value first derivative', 'answer_start': 19}]}, {'question': 'What characteristic is observed at the ideal edge in the first derivative of I(x)?', 'id': 'unique_question_identifier_010', 'answers': [{'text': 'Has a peak at the edge', 'answer_start': 199}]}]}, {'context': 'Second derivative of I(x) has a zero crossing at the edge. Ideal edge Real edge First derivative shows peak. Second derivative shows zero crossing.', 'qas': [{'question': 'What does the second derivative of I(x) indicate at the edge?', 'id': 'unique_question_identifier_011', 'answers': [{'text': 'Has a zero crossing at the edge', 'answer_start': 33}]}]}, {'context': 'Slopes of Discrete Functions: Left and right slope may not be the same. Solution? Take average of left and right slope. Computing Derivative of Discrete Function: Actual slope (solid line), Estimated slope (dashed line).', 'qas': [{'question': 'How can the slope of discrete functions be normalized?', 'id': 'unique_question_identifier_012', 'answers': [{'text': 'Take average of left and right slope', 'answer_start': 71}]}, {'question': 'What are the two types of slopes in discrete functions?', 'id': 'unique_question_identifier_013', 'answers': [{'text': 'Actual slope (solid line), Estimated slope (dashed line)', 'answer_start': 120}]}]}, {'context': 'Finite Differences: Forward difference (right slope), Backward difference (left slope), Central Difference (average slope). Definition: Function Gradient: Let f(x,y) be a 2D function.', 'qas': [{'question': 'What are the three types of finite differences?', 'id': 'unique_question_identifier_014', 'answers': [{'text': 'Forward difference (right slope), Backward difference (left slope), Central Difference (average slope)', 'answer_start': 18}]}, {'question': 'What is the context for defining a function gradient?', 'id': 'unique_question_identifier_015', 'answers': [{'text': '2D function f(x,y)', 'answer_start': 143}]}]}, {'context': 'Gradient: Vector whose direction is in direction of maximum rate of change of f and whose magnitude is maximum rate of change of f. Gradient is perpendicular to edge contour. Image Gradient: Image is 2D discrete function.', 'qas': [{'question': 'What does the gradient vector represent in the context of a function?', 'id': 'unique_question_identifier_016', 'answers': [{'text': 'Direction is in direction of maximum rate of change of f and magnitude is maximum rate of change of f', 'answer_start': 9}]}, {'question': 'What is the orientation of the gradient relative to the edge contour?', 'id': 'unique_question_identifier_017', 'answers': [{'text': 'Perpendicular to edge contour', 'answer_start': 137}]}]}, {'context': 'Image derivatives in horizontal and vertical directions. Image gradient at location (u,v). Gradient magnitude: Magnitude is invariant under image rotation, used in edge detection. Derivative Filters: Recall that we can compute derivative of discrete function.', 'qas': [{'question': 'What properties do image derivatives have in different directions?', 'id': 'unique_question_identifier_018', 'answers': [{'text': 'Horizontal and vertical directions', 'answer_start': 26}]}, {'question': 'What is the significance of the gradient magnitude in image gradients?', 'id': 'unique_question_identifier_019', 'answers': [{'text': 'Invariant under image rotation, used in edge detection', 'answer_start': 99}]}]}, {'context': 'Can we make linear filter that computes central differences. Finite Differences as Convolutions: Forward difference: Take a convolution kernel.', 'qas': [{'question': 'What is the goal in creating a linear filter in the context of image processing?', 'id': 'unique_question_identifier_020', 'answers': [{'text': 'To compute central differences', 'answer_start': 28}]}, {'question': 'What method is used in finite differences to calculate forward difference?', 'id': 'unique_question_identifier_021', 'answers': [{'text': 'Take a convolution kernel', 'answer_start': 127}]}]}, {'context': 'Finite Differences as Convolutions: Central difference. Convolution kernel is: Notice: Derivative kernels sum to zero. xDerivative of Image using Central Difference.', 'qas': [{'question': 'What is the characteristic of derivative kernels in the context of convolutions?', 'id': 'unique_question_identifier_022', 'answers': [{'text': 'Derivative kernels sum to zero', 'answer_start': 83}]}, {'question': 'What method is used for calculating the xDerivative of an image?', 'id': 'unique_question_identifier_023', 'answers': [{'text': 'Central Difference', 'answer_start': 123}]}]}, {'context': 'yDerivative of Image using Central Difference. Derivative Filters: A synthetic image. Magnitude of gradient. Gradient slope in vertical and horizontal direction.', 'qas': [{'question': 'What method is used for calculating the yDerivative of an image?', 'id': 'unique_question_identifier_024', 'answers': [{'text': 'Central Difference', 'answer_start': 33}]}, {'question': 'What are the features analyzed by derivative filters?', 'id': 'unique_question_identifier_025', 'answers': [{'text': 'Magnitude of gradient, gradient slope in vertical and horizontal direction', 'answer_start': 56}]}]}, {'context': \"Partial Image Derivatives: Partial derivatives of images replaced by finite differences. Alternatives are: Robert's gradient, Prewitt, Sobel. Using Averaging with Derivatives: Finite difference operator is sensitive to noise.\", 'qas': [{'question': 'What are the alternatives for computing partial image derivatives?', 'id': 'unique_question_identifier_026', 'answers': [{'text': \"Robert's gradient, Prewitt, Sobel\", 'answer_start': 83}]}, {'question': 'Why are derivatives averaged in the context of image processing?', 'id': 'unique_question_identifier_027', 'answers': [{'text': 'Finite difference operator is sensitive to noise', 'answer_start': 185}]}]}, {'context': 'Derivates more robust if derivative computations are averaged in a neighborhood. Prewitt operator: derivative in x, then average in y. y derivative kernel, defined similarly. Average in y direction, Derivative in x direction.', 'qas': [{'question': 'What makes derivatives more robust in image processing?', 'id': 'unique_question_identifier_028', 'answers': [{'text': 'Averaging derivative computations in a neighborhood', 'answer_start': 0}]}, {'question': 'How is the Prewitt operator applied in image processing?', 'id': 'unique_question_identifier_029', 'answers': [{'text': 'Derivative in x, then average in y', 'answer_start': 92}]}]}, {'context': 'Note: Filter kernel is flipped in convolution. Sobel Operator: Similar to Prewitt, but averaging kernel is higher in middle. Average in x direction, Derivative in y direction. Note: Filter kernel is flipped in convolution.', 'qas': [{'question': 'How does the Sobel operator differ from the Prewitt operator?', 'id': 'unique_question_identifier_030', 'answers': [{'text': 'Averaging kernel is higher in the middle in Sobel operator', 'answer_start': 59}]}, {'question': 'What is a common procedural note in applying convolution kernels?', 'id': 'unique_question_identifier_031', 'answers': [{'text': 'Filter kernel is flipped in convolution', 'answer_start': 0}]}]}, {'context': 'Prewitt and Sobel Edge Operators: Prewitt Operator, Sobel Operator. Written in separable form. Gradient-Based Edge Detection: Compute image derivatives by convolution.', 'qas': [{'question': 'What are the two types of edge operators mentioned?', 'id': 'unique_question_identifier_032', 'answers': [{'text': 'Prewitt Operator, Sobel Operator', 'answer_start': 29}]}, {'question': 'What is the first step in gradient-based edge detection?', 'id': 'unique_question_identifier_033', 'answers': [{'text': 'Compute image derivatives by convolution', 'answer_start': 105}]}]}, {'context': 'Scaled Filter results. Compute edge gradient magnitude. Compute edge gradient direction. Typical process of Gradient-based edge detection.', 'qas': [{'question': 'What follows the computation of image derivatives in gradient-based edge detection?', 'id': 'unique_question_identifier_034', 'answers': [{'text': 'Compute edge gradient magnitude and edge gradient direction', 'answer_start': 19}]}, {'question': 'What characterizes the typical process of gradient-based edge detection?', 'id': 'unique_question_identifier_035', 'answers': [{'text': 'Scaled Filter results, compute edge gradient magnitude, compute edge gradient direction', 'answer_start': 0}]}]}, {'context': 'References: Wilhelm Burger and Mark J. Burge, Digital Image Processing, Springer, 2008. University of Utah, CS 4640: Image Processing Basics, Spring 2012. Rutgers University, CS 334, Introduction to Imaging and Multimedia, Fall2012.', 'qas': [{'question': \"Who are the authors of the referenced book 'Digital Image Processing'?\", 'id': 'unique_question_identifier_036', 'answers': [{'text': 'Wilhelm Burger and Mark J. Burge', 'answer_start': 12}]}, {'question': 'What universities provided courses referenced in the material?', 'id': 'unique_question_identifier_037', 'answers': [{'text': 'University of Utah, Rutgers University', 'answer_start': 85}]}]}, {'context': 'Digital Image Processing Lecture 5: Morphological Filters Dr. Belhassen Akrout Computer Science Dept. Mathematical Morphology: Originally operated on Binary (black and white) images. Binary images like Faxes, digitally printed images, obtained from thresholding grayscale images.', 'qas': [{'question': 'What is the main focus of Lecture 5 in Digital Image Processing?', 'id': 'unique_question_identifier_038', 'answers': [{'text': 'Morphological Filters', 'answer_start': 34}]}, {'question': 'What type of images does Mathematical Morphology primarily operate on?', 'id': 'unique_question_identifier_039', 'answers': [{'text': 'Binary (black and white) images', 'answer_start': 107}]}]}, {'context': \"Morphological filters alter local structures in an image. Translation: A is set of pixels in binary image, w = (x,y) is a particular coordinate point, A 'translated' in direction (x,y). Reflection: A is set of pixels, Reflection of A is given.\", 'qas': [{'question': 'What is the purpose of morphological filters in image processing?', 'id': 'unique_question_identifier_040', 'answers': [{'text': 'Alter local structures in an image', 'answer_start': 21}]}, {'question': \"What are the concepts of 'translation' and 'reflection' in the context of mathematical morphology?\", 'id': 'unique_question_identifier_041', 'answers': [{'text': \"'Translation' refers to moving a set of pixels in a direction, 'reflection' is about flipping a set of pixels\", 'answer_start': 63}]}]}, {'context': 'Mathematical Morphology: 2 basic mathematical morphology operations, built from translations and reflections: Dilation and Erosion. Composite relations include Closing and Opening, Conditional Dilation.', 'qas': [{'question': 'What are the two basic operations in mathematical morphology?', 'id': 'unique_question_identifier_042', 'answers': [{'text': 'Dilation and Erosion', 'answer_start': 92}]}, {'question': 'What composite relations are used in mathematical morphology?', 'id': 'unique_question_identifier_043', 'answers': [{'text': 'Closing and Opening, Conditional Dilation', 'answer_start': 131}]}]}, {'context': 'Dilation expands connected sets of 1s of a binary image. It can be used for growing features, filling holes and gaps. Erosion shrinks connected sets of 1s in a binary image. It can be used for shrinking features, removing bridges, branches and small protrusions.', 'qas': [{'question': 'What is the purpose of dilation in mathematical morphology?', 'id': 'unique_question_identifier_044', 'answers': [{'text': 'Expands connected sets of 1s of a binary image, used for growing features, filling holes and gaps', 'answer_start': 0}]}, {'question': 'What does erosion accomplish in mathematical morphology?', 'id': 'unique_question_identifier_045', 'answers': [{'text': 'Shrinks connected sets of 1s in a binary image, used for shrinking features, removing bridges, branches, and small protrusions', 'answer_start': 108}]}]}, {'context': 'Shrink and Let Grow: Shrinking removes border pixels, Growing adds a layer of pixels at the border. Image structures are iteratively shrunk by peeling off a layer of thickness (layer of pixel) at boundaries.', 'qas': [{'question': \"What are the processes of 'Shrink' and 'Let Grow' in morphological operations?\", 'id': 'unique_question_identifier_046', 'answers': [{'text': 'Shrinking removes border pixels, Growing adds a layer of pixels at the border', 'answer_start': 18}]}, {'question': 'How is the iterative process of shrinking executed in image structures?', 'id': 'unique_question_identifier_047', 'answers': [{'text': 'By peeling off a layer of thickness at boundaries', 'answer_start': 93}]}]}, {'context': 'Shrinking removes smaller structures, leaving only large structures. Remaining structures are then grown back by the same amount. Eventually, large structures return to original size while smaller regions disappear. Basic Morphological Operations: 4Neighborhood (N4) and 8Neighborhood (N8).', 'qas': [{'question': 'What is the outcome of the shrinking and growing process in morphological operations?', 'id': 'unique_question_identifier_048', 'answers': [{'text': 'Large structures return to original size, smaller regions disappear', 'answer_start': 100}]}, {'question': 'What are the definitions of 4Neighborhood and 8Neighborhood in morphological operations?', 'id': 'unique_question_identifier_049', 'answers': [{'text': '4Neighborhood (N4): 4 pixels adjacent to a given pixel; 8Neighborhood (N8): N4 + 4 diagonal adjacent pixels', 'answer_start': 186}]}]}, {'context': 'Dilation: Suppose A and B are sets of pixels, dilation of A by B. Also called Minkowski addition. Meaning? Replace every pixel in A with a copy of B (or vice versa). For every pixel x in B, translate A by x, take union of all these translations.', 'qas': [{'question': 'What does dilation involve in the context of morphological operations?', 'id': 'unique_question_identifier_050', 'answers': [{'text': 'Replacing every pixel in A with a copy of B and vice versa, then translating A by each pixel in B and taking the union of these translations', 'answer_start': 0}]}]}, {'context': 'Dilation Example: For A and B shown below, translation of A by (1,1). Dilation Example: Union of all translations.', 'qas': [{'question': 'What is illustrated in the dilation example?', 'id': 'unique_question_identifier_051', 'answers': [{'text': 'Translation of A by (1,1) and the union of all translations', 'answer_start': 37}]}]}, {'context': 'Another Dilation Example: Dilation increases the size of structure. A and B do not have to overlap. Example: For the same A, if we change B to dilation. We usually assume A is being processed, B is a smaller set of pixels, called the structuring element.', 'qas': [{'question': 'What effect does dilation have on a structure in morphological operations?', 'id': 'unique_question_identifier_052', 'answers': [{'text': 'Increases the size of the structure', 'answer_start': 25}]}, {'question': 'What is the role of B in the context of dilation?', 'id': 'unique_question_identifier_053', 'answers': [{'text': 'A smaller set of pixels, called the structuring element', 'answer_start': 128}]}]}, {'context': 'The Structuring Element: A structuring element is a shape mask used in basic morphological operations. They can be any shape and size that is digitally representable, and each has an origin. The Structuring Element somewhat similar to a filter.', 'qas': [{'question': 'What is a structuring element in morphological operations?', 'id': 'unique_question_identifier_054', 'answers': [{'text': 'A shape mask used in basic morphological operations, can be any digitally representable shape and size', 'answer_start': 20}]}, {'question': 'How does a structuring element compare to a filter?', 'id': 'unique_question_identifier_055', 'answers': [{'text': 'Somewhat similar to a filter', 'answer_start': 194}]}]}, {'context': 'Erosion: Given sets A and B, the erosion of A by B. Find all occurrences of B in A. Example: 1 occurrence of B in A. Erosion: All occurrences of B in A. For each occurrence, mark center of B. Erosion: union of center of all occurrences of B in A.', 'qas': [{'question': 'How is erosion executed in morphological operations?', 'id': 'unique_question_identifier_056', 'answers': [{'text': 'Find all occurrences of B in A, mark the center of B for each occurrence, and take the union of these centers', 'answer_start': 0}]}]}, {'context': 'Erosion is related to Minkowski subtraction. Erosion and dilation are inverses of each other.', 'qas': [{'question': 'What mathematical concept is erosion related to in morphological operations?', 'id': 'unique_question_identifier_057', 'answers': [{'text': 'Minkowski subtraction', 'answer_start': 16}]}, {'question': 'What is the relationship between erosion and dilation in morphological operations?', 'id': 'unique_question_identifier_058', 'answers': [{'text': 'They are inverses of each other', 'answer_start': 56}]}]}, {'context': 'An Application: Boundary Detection. Given an image A and structuring element B, we can define external boundary. Dilation of image A minus erosion image A (by structuring element B) defines morphological gradient.', 'qas': [{'question': 'How is external boundary defined in the application of boundary detection?', 'id': 'unique_question_identifier_059', 'answers': [{'text': 'Dilation of image A minus erosion of image A by structuring element B', 'answer_start': 101}]}, {'question': 'What constitutes the morphological gradient in boundary detection?', 'id': 'unique_question_identifier_060', 'answers': [{'text': 'Dilation - erosion', 'answer_start': 148}]}]}, {'context': 'Example: Internal Boundary of Binary Image. We can also define internal boundary. Example: External Boundary and Morphological Gradient Image. External Boundary and Morphological Gradient. Example: Extraction of Boundary Pixels using Morphological Operations.', 'qas': [{'question': 'What is an example of using morphological operations for boundary extraction?', 'id': 'unique_question_identifier_061', 'answers': [{'text': 'Extraction of Boundary Pixels', 'answer_start': 196}]}]}, {'context': \"Properties of Dilation: Dilation operation is commutative and associative. This implies, as with separable filters, it's more efficient to apply a large structuring element as a sequence of smaller structuring elements.\", 'qas': [{'question': 'What are the key properties of the dilation operation in morphological filters?', 'id': 'unique_question_identifier_062', 'answers': [{'text': 'Commutative and associative', 'answer_start': 21}]}]}, {'context': 'Properties of Erosion: Erosion is not commutative. If erosion and dilation are combined, this chain rule holds: Dilation of foreground equals inverting erosion of background.', 'qas': [{'question': 'How does erosion differ from dilation in terms of properties?', 'id': 'unique_question_identifier_063', 'answers': [{'text': 'Erosion is not commutative', 'answer_start': 21}]}, {'question': 'What is the chain rule for combining erosion and dilation?', 'id': 'unique_question_identifier_064', 'answers': [{'text': 'Dilation of foreground equals inverting erosion of background', 'answer_start': 91}]}]}, {'context': 'Designing Morphological Filters: A morphological filter is specified by the type of operation (e.g., dilation, erosion) and contents of the structuring element. In practice, quasi circular shaped structuring elements are used. Dilation with a circular structuring element of radius r adds thickness r. Erosion with a circular structuring element of radius r removes thickness r.', 'qas': [{'question': 'What are the key considerations in designing morphological filters?', 'id': 'unique_question_identifier_065', 'answers': [{'text': 'Type of operation and contents of the structuring element', 'answer_start': 29}]}, {'question': 'What is the effect of using a circular structuring element in dilation and erosion?', 'id': 'unique_question_identifier_066', 'answers': [{'text': 'Dilation adds thickness r, erosion removes thickness r', 'answer_start': 223}]}]}, {'context': 'Dilation and Erosion using Different Structuring Elements. Example: Composing Large Filters by Repeatedly Applying Smaller Filters. More efficient E.g., composing Isotropic filter.', 'qas': [{'question': 'How can large filters be efficiently composed in the context of dilation and erosion?', 'id': 'unique_question_identifier_067', 'answers': [{'text': 'By repeatedly applying smaller filters', 'answer_start': 73}]}, {'question': 'What is an example of a filter composed by this method?', 'id': 'unique_question_identifier_068', 'answers': [{'text': 'Isotropic filter', 'answer_start': 135}]}]}, {'context': 'References: Wilhelm Burger and Mark J. Burge, Digital Image Processing, Springer, 2008. University of Utah, CS 4640: Image Processing Basics, Spring 2012. Rutgers University, CS 334, Introduction to Imaging and Multimedia, Fall2012. Gonzales and Woods, Digital Image Processing (3rd edition), Prentice Hall.', 'qas': [{'question': \"Who authored the book 'Digital Image Processing' referenced in the material?\", 'id': 'unique_question_identifier_069', 'answers': [{'text': 'Wilhelm Burger and Mark J. Burge', 'answer_start': 12}]}, {'question': \"Which edition of 'Digital Image Processing' by Gonzales and Woods is referenced?\", 'id': 'unique_question_identifier_070', 'answers': [{'text': '3rd edition', 'answer_start': 265}]}]}, {'context': \"Many consider Gilligan's ethics of care to be the best attempt so far, working well in family contexts. Extending the ethics of care into larger spheres encounters difficulties. Gilligan emphasizes the moral urgency of partiality. An example is a mother's partiality to her children.\", 'qas': [{'question': \"What is considered a successful application of Gilligan's ethics of care?\", 'id': 'unique_question_identifier_071', 'answers': [{'text': 'In family contexts', 'answer_start': 55}]}, {'question': 'What moral aspect does Gilligan emphasize in her ethics of care?', 'id': 'unique_question_identifier_072', 'answers': [{'text': 'The moral urgency of partiality', 'answer_start': 124}]}]}, {'context': 'Impartiality is insisted upon in many situations, as in a lawsuit example. People we have never met can exercise moral demands upon us. For instance, we believe a person imprisoned in another country should not be tortured, and perhaps we should help if possible.', 'qas': [{'question': 'What is an example where impartiality is crucial?', 'id': 'unique_question_identifier_073', 'answers': [{'text': 'In a lawsuit', 'answer_start': 0}]}, {'question': 'What is a moral belief commonly held about people we have never met?', 'id': 'unique_question_identifier_074', 'answers': [{'text': 'They should not be subjected to torture', 'answer_start': 97}]}]}, {'context': \"Nietzsche's proclamation 'God is dead' signified a shift in how we view ourselves in the universe. He suggested the Judeo-Christian tradition no longer serves our values as it once did. Nietzsche promoted the idea of pluralism, which posits many goods and truths.\", 'qas': [{'question': \"What did Nietzsche's statement 'God is dead' imply?\", 'id': 'unique_question_identifier_075', 'answers': [{'text': 'A shift in how we view ourselves in the universe', 'answer_start': 37}]}, {'question': 'What concept did Nietzsche advocate?', 'id': 'unique_question_identifier_076', 'answers': [{'text': 'Pluralism', 'answer_start': 329}]}]}, {'context': \"Pluralism opposes Plato's view that all good things share a common quality. Pluralists argue for transcultural and transhistorical human nature and values. They seek concrete goods and practices that enrich human life, avoiding the dichotomy between Plato's absolutism and moral relativism.\", 'qas': [{'question': \"What does pluralism oppose in Plato's philosophy?\", 'id': 'unique_question_identifier_077', 'answers': [{'text': 'The idea that all good things share a common quality', 'answer_start': 0}]}, {'question': 'What do pluralists search for in human life?', 'id': 'unique_question_identifier_078', 'answers': [{'text': 'Concrete goods and practices that enrich human life', 'answer_start': 170}]}]}, {'context': 'Pluralists emphasize the importance of investigating and questioning current cultural practices and their impact on human flourishing. They inquire whether aspects like the American attitude toward sexuality are enhancing or diminishing the human condition.', 'qas': [{'question': 'What do pluralists emphasize in their approach to ethics?', 'id': 'unique_question_identifier_079', 'answers': [{'text': 'Investigating and questioning current cultural practices', 'answer_start': 0}]}, {'question': 'What kind of inquiry do pluralists engage in?', 'id': 'unique_question_identifier_080', 'answers': [{'text': 'Inquiring whether cultural practices improve or interfere with human flourishing', 'answer_start': 72}]}]}, {'context': 'The American attitude towards sexuality raises questions: Is there a singular attitude or are there multiple? This leads to the broader ethical contribution to the history of philosophy.', 'qas': [{'question': 'What question is raised about the American attitude towards sexuality?', 'id': 'unique_question_identifier_081', 'answers': [{'text': 'Is there a singular American attitude towards sexuality, or are there multiple attitudes?', 'answer_start': 0}]}, {'question': 'What does this question about attitudes towards sexuality contribute to?', 'id': 'unique_question_identifier_082', 'answers': [{'text': 'The broader ethical contribution to the history of philosophy', 'answer_start': 128}]}]}, {'context': 'Cyberterrorist: An individual who launches computer-based attacks against other computers or networks to intimidate or coerce a government for political or social objectives. Data breach: The unintended release of sensitive data or access by unauthorized individuals.', 'qas': [{'question': 'What is a cyberterrorist?', 'id': 'unique_question_identifier_083', 'answers': [{'text': 'An individual who launches computer-based attacks to intimidate or coerce a government for political or social objectives', 'answer_start': 0}]}, {'question': 'What constitutes a data breach?', 'id': 'unique_question_identifier_084', 'answers': [{'text': 'The unintended release of sensitive data or access by unauthorized individuals', 'answer_start': 162}]}]}, {'context': 'Decision support system (DSS): A business information system to improve decision making in industries. Defamation: Making false statements that harm another person. Deliverables: The products of a software development process.', 'qas': [{'question': 'What is a decision support system (DSS)?', 'id': 'unique_question_identifier_085', 'answers': [{'text': 'A business information system to improve decision making in industries', 'answer_start': 0}]}, {'question': 'What are deliverables in a software development process?', 'id': 'unique_question_identifier_086', 'answers': [{'text': 'The products of a software development process', 'answer_start': 158}]}]}, {'context': 'Digital divide: The gap between those with and without access to modern information and communications technology. DMCA: An act that implements WIPO treaties in the US and prohibits circumventing technological protection.', 'qas': [{'question': 'What is the digital divide?', 'id': 'unique_question_identifier_087', 'answers': [{'text': 'The gap between those with and without access to modern information and communications technology', 'answer_start': 0}]}, {'question': 'What is the purpose of the DMCA?', 'id': 'unique_question_identifier_088', 'answers': [{'text': 'Implements WIPO treaties in the US and prohibits circumventing technological protection', 'answer_start': 134}]}]}, {'context': 'Distributed denial-of-service attack (DDoS): A malicious hacker overloads a target site with data requests. Doxing: Revealing the identity of an anonymous poster through Internet records. Duty of care: The obligation to protect people from harm or risk.', 'qas': [{'question': 'What is a distributed denial-of-service attack (DDoS)?', 'id': 'unique_question_identifier_089', 'answers': [{'text': 'A malicious hacker overloads a target site with data requests', 'answer_start': 0}]}, {'question': \"What is the principle of 'duty of care'?\", 'id': 'unique_question_identifier_090', 'answers': [{'text': 'The obligation to protect people from harm or risk', 'answer_start': 157}]}]}, {'context': \"Electronic Communications Privacy Act of 1986 (ECPA) focuses on the protection of communications during transfer and in electronic storage. E-discovery: The process of using electronically stored information in legal actions. Electronic health record (EHR): A digital record of an individual's health information.\", 'qas': [{'question': 'What does the Electronic Communications Privacy Act of 1986 (ECPA) focus on?', 'id': 'unique_question_identifier_091', 'answers': [{'text': 'The protection of communications during transfer and in electronic storage', 'answer_start': 0}]}, {'question': 'What is an electronic health record (EHR)?', 'id': 'unique_question_identifier_092', 'answers': [{'text': \"A digital record of an individual's health information\", 'answer_start': 200}]}]}, {'context': 'Integration testing: A form of software testing where individual software units are combined into an integrated subsystem. The subsystem undergoes rigorous testing to ensure that the linkages among the various subsystems work successfully.', 'qas': [{'question': 'What is integration testing in software development?', 'id': 'unique_question_identifier_083', 'answers': [{'text': 'Testing where individual software units are combined and tested to ensure their linkages work', 'answer_start': 26}]}]}, {'context': 'Integrity: Adherence to a personal code of principles. Intellectual property: Works of the mind like art, books, films, which are protected through copyright, patent, trade secret, and trademark laws.', 'qas': [{'question': \"What is meant by 'integrity' in a personal or professional context?\", 'id': 'unique_question_identifier_084', 'answers': [{'text': 'Adherence to a personal code of principles', 'answer_start': 19}]}, {'question': 'What constitutes intellectual property?', 'id': 'unique_question_identifier_085', 'answers': [{'text': 'Works of the mind like art, books, films, protected through various laws', 'answer_start': 55}]}]}, {'context': \"Intentional misrepresentation: Fraud that occurs when a product's quality is misrepresented or a defect is concealed. Internet censorship: Control or suppression of publishing or accessing information on the Internet.\", 'qas': [{'question': \"What does 'intentional misrepresentation' involve?\", 'id': 'unique_question_identifier_086', 'answers': [{'text': \"Fraud involving misrepresentation of a product's quality or concealing a defect\", 'answer_start': 0}]}, {'question': 'What is internet censorship?', 'id': 'unique_question_identifier_087', 'answers': [{'text': 'Control or suppression of publishing or accessing information on the Internet', 'answer_start': 150}]}]}, {'context': 'Intrusion detection system (IDS): Software and/or hardware monitoring system and network resources. It notifies network security personnel of possible intrusions or misuse. IT user: A person for whom a hardware or software product is designed.', 'qas': [{'question': 'What is the function of an intrusion detection system (IDS)?', 'id': 'unique_question_identifier_088', 'answers': [{'text': 'Monitoring system and network resources for intrusions or misuse', 'answer_start': 31}]}, {'question': 'Who is considered an IT user?', 'id': 'unique_question_identifier_089', 'answers': [{'text': 'A person for whom a hardware or software product is designed', 'answer_start': 193}]}]}, {'context': 'Electronic Industry Citizenship Coalition (EICC): An organization promoting a common code of conduct for the electronics and ICT industry. Electronic Product Environmental Assessment Tool (EPEAT): A system evaluating electronic products based on environmental criteria.', 'qas': [{'question': 'What is the purpose of the Electronic Industry Citizenship Coalition (EICC)?', 'id': 'unique_question_identifier_090', 'answers': [{'text': 'Promoting a common code of conduct for the electronics and ICT industry', 'answer_start': 37}]}, {'question': 'What does the Electronic Product Environmental Assessment Tool (EPEAT) do?', 'id': 'unique_question_identifier_091', 'answers': [{'text': 'Evaluates electronic products based on environmental criteria', 'answer_start': 228}]}]}, {'context': 'Problem statement: A clear, concise description of the issue that needs to be addressed in a decision-making process. Product liability: The liability of manufacturers, sellers, lessors, and others for injuries caused by defective products.', 'qas': [{'question': 'What is a problem statement in decision-making?', 'id': 'unique_question_identifier_092', 'answers': [{'text': 'A clear, concise description of the issue that needs to be addressed', 'answer_start': 0}]}, {'question': 'What does product liability refer to?', 'id': 'unique_question_identifier_093', 'answers': [{'text': 'The liability of manufacturers, sellers, lessors for injuries caused by defective products', 'answer_start': 105}]}]}, {'context': 'Professional code of ethics: A statement of principles and core values essential to a particular occupational group. Professional malpractice: Breach of duty of care by a professional.', 'qas': [{'question': 'What is a professional code of ethics?', 'id': 'unique_question_identifier_094', 'answers': [{'text': 'A statement of principles and core values essential to an occupational group', 'answer_start': 0}]}, {'question': 'What constitutes professional malpractice?', 'id': 'unique_question_identifier_095', 'answers': [{'text': 'Breach of the duty of care by a professional', 'answer_start': 149}]}]}, {'context': 'Quality assurance (QA): Methods within the software development cycle designed to guarantee reliable operation of a product. Quality management: Business practices focusing on defining, measuring, and refining the quality of development processes and products.', 'qas': [{'question': 'What is quality assurance (QA) in software development?', 'id': 'unique_question_identifier_096', 'answers': [{'text': 'Methods designed to guarantee reliable operation of a product', 'answer_start': 0}]}, {'question': 'What does quality management in business involve?', 'id': 'unique_question_identifier_097', 'answers': [{'text': 'Focusing on defining, measuring, and refining the quality of processes and products', 'answer_start': 160}]}]}, {'context': 'Ransomware: Malware that disables a computer or smartphone until the victim pays a fee or ransom. Reasonable assurance: A concept in computer security that managers must ensure the cost of control does not exceed the system’s benefits or the risks involved.', 'qas': [{'question': 'What is ransomware?', 'id': 'unique_question_identifier_098', 'answers': [{'text': 'Malware that disables a computer or smartphone until a ransom is paid', 'answer_start': 0}]}, {'question': 'What is the concept of reasonable assurance in computer security?', 'id': 'unique_question_identifier_099', 'answers': [{'text': 'Ensuring the cost of control does not exceed the system’s benefits or risks', 'answer_start': 117}]}]}, {'context': 'Reliability: The probability of a component or system performing without failure over its product life. Résumé inflation: Falsely claiming competence in a skill, usually because that skill is in high demand.', 'qas': [{'question': 'What does reliability mean in the context of a product?', 'id': 'unique_question_identifier_100', 'answers': [{'text': 'The probability of a component or system performing without failure over its product life', 'answer_start': 0}]}, {'question': 'What is résumé inflation?', 'id': 'unique_question_identifier_101', 'answers': [{'text': 'Falsely claiming competence in a skill, often in high demand', 'answer_start': 118}]}]}]}]}\n",
            "\n",
            "Inspecting Test Dataset:\n",
            "Item 0: {'data': [{'paragraphs': [{'context': \"Kant maintains that it is always irrational and wrong to lie, even to save an innocent life. This raises questions about the morality of lying in extreme circumstances, such as saving a child's life or lying to Nazis to protect Jews.\", 'qas': [{'question': \"What is Kant's position on lying?\", 'id': 'unique_question_identifier_102', 'answers': [{'text': 'It is always irrational and wrong, even to save an innocent life', 'answer_start': 11}]}, {'question': \"What ethical dilemma arises from Kant's position on lying?\", 'id': 'unique_question_identifier_103', 'answers': [{'text': \"The morality of lying in extreme circumstances, like saving a child's life or protecting Jews from Nazis\", 'answer_start': 99}]}]}, {'context': \"Kant's theory suggests that actions motivated by inclination towards happiness are not moral actions. This contrasts with the idea of moral duty, leading to the question of whether only miserable people can be moral.\", 'qas': [{'question': 'How does Kant view actions motivated by the pursuit of happiness?', 'id': 'unique_question_identifier_104', 'answers': [{'text': 'Not as moral actions, since they are not motivated by duty', 'answer_start': 71}]}, {'question': \"What issue arises from Kant's distinction between moral duty and happiness?\", 'id': 'unique_question_identifier_105', 'answers': [{'text': 'Whether only miserable people can be considered moral', 'answer_start': 203}]}]}, {'context': 'Utilitarianism, responding to deontology, posits that morality and happiness are not opposites, but the same. This concept, originating from Epicurean hedonism, was developed by Jeremy Bentham into what is known as utilitarianism.', 'qas': [{'question': 'What is the main idea of utilitarianism in response to deontology?', 'id': 'unique_question_identifier_106', 'answers': [{'text': 'Morality and happiness are not opposites, but the same', 'answer_start': 103}]}, {'question': 'Who developed the principles of Epicurean hedonism into utilitarianism?', 'id': 'unique_question_identifier_107', 'answers': [{'text': 'Jeremy Bentham', 'answer_start': 392}]}]}, {'context': \"Temporary workers are subject to specific restrictions: they cannot use company-owned athletic fields, park in the company lot, buy goods at the company store, eat in the cafeteria, join company social clubs, attend parties or company meetings, and their email addresses must have an 'n-' prefix. As a new HR manager, you are tasked to respond to complaints from these workers about working conditions.\", 'qas': [{'question': 'What restrictions are placed on temporary workers in the company?', 'id': 'unique_question_identifier_108', 'answers': [{'text': \"Cannot use company facilities, park in the lot, buy at the store, eat in the cafeteria, join clubs, attend parties/meetings, and must have 'n-' prefix in email\", 'answer_start': 45}]}, {'question': 'As an HR manager, what complaints might you expect from temporary workers?', 'id': 'unique_question_identifier_109', 'answers': [{'text': 'Complaints about working conditions due to the specific restrictions', 'answer_start': 364}]}]}, {'context': 'Catalytic Software, an IT firm, has built a self-contained community near Hyderabad, India, to accommodate software developers and their families. This community aims to overcome barriers faced by large-scale tech businesses in India, providing homes, amenities, and efficient work conditions. A job offer at Catalytic Software involves a year-long assignment in this community before managing U.S.-based projects.', 'qas': [{'question': 'What is the purpose of the self-contained community built by Catalytic Software near Hyderabad?', 'id': 'unique_question_identifier_110', 'answers': [{'text': 'To accommodate software developers and families, overcoming barriers faced by large-scale tech businesses in India', 'answer_start': 82}]}, {'question': 'What are the conditions and expectations of the job offer at Catalytic Software?', 'id': 'unique_question_identifier_111', 'answers': [{'text': 'A year-long assignment in the community near Hyderabad before managing U.S.-based projects', 'answer_start': 480}]}]}]}]}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2Tokenizer\n",
        "\n",
        "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
        "tokenizer.pad_token = tokenizer.eos_token  # Use the EOS token for padding\n",
        "\n",
        "def process_data(dataset_split):\n",
        "    tokenized_examples = []\n",
        "    for entry in dataset_split:\n",
        "        for item in entry['data']:\n",
        "            for paragraph in item['paragraphs']:\n",
        "                context = paragraph['context']\n",
        "                for qa in paragraph['qas']:\n",
        "                    question = qa['question']\n",
        "                    answer = qa['answers'][0]['text']\n",
        "                    input_text = f\"question: {question} context: {context} answer: {answer}\"\n",
        "                    tokenized_input = tokenizer(input_text, truncation=True, padding='max_length', max_length=1024, return_tensors='pt')\n",
        "                    tokenized_input['labels'] = tokenized_input.input_ids.clone()\n",
        "                    tokenized_examples.append(tokenized_input)\n",
        "    return tokenized_examples\n",
        "\n",
        "\n",
        "# Assuming dataset is already loaded using load_dataset\n",
        "train_data = process_data(dataset['train'])\n",
        "test_data = process_data(dataset['test'])\n"
      ],
      "metadata": {
        "id": "ysqnpO1f2fl1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DdY0g3KgpBhU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "3434abbd-47c7-41b1-fe5d-932c24585421"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                      attention_mask  \\\n",
              "0  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
              "1  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
              "2  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
              "3  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
              "4  [[tensor(1), tensor(1), tensor(1), tensor(1), ...   \n",
              "\n",
              "                                           input_ids  \\\n",
              "0  [[tensor(25652), tensor(25), tensor(1867), ten...   \n",
              "1  [[tensor(25652), tensor(25), tensor(1867), ten...   \n",
              "2  [[tensor(25652), tensor(25), tensor(1374), ten...   \n",
              "3  [[tensor(25652), tensor(25), tensor(1867), ten...   \n",
              "4  [[tensor(25652), tensor(25), tensor(1867), ten...   \n",
              "\n",
              "                                              labels  \n",
              "0  [[tensor(25652), tensor(25), tensor(1867), ten...  \n",
              "1  [[tensor(25652), tensor(25), tensor(1867), ten...  \n",
              "2  [[tensor(25652), tensor(25), tensor(1374), ten...  \n",
              "3  [[tensor(25652), tensor(25), tensor(1867), ten...  \n",
              "4  [[tensor(25652), tensor(25), tensor(1867), ten...  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-50de6a11-8bc7-408e-aaea-6430a7c1baa0\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>attention_mask</th>\n",
              "      <th>input_ids</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1867), ten...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1867), ten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1867), ten...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1867), ten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1374), ten...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1374), ten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1867), ten...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1867), ten...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>[[tensor(1), tensor(1), tensor(1), tensor(1), ...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1867), ten...</td>\n",
              "      <td>[[tensor(25652), tensor(25), tensor(1867), ten...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-50de6a11-8bc7-408e-aaea-6430a7c1baa0')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-50de6a11-8bc7-408e-aaea-6430a7c1baa0 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-50de6a11-8bc7-408e-aaea-6430a7c1baa0');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-37afeae4-f949-4de3-9bd3-79e44bae7cf0\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-37afeae4-f949-4de3-9bd3-79e44bae7cf0')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-37afeae4-f949-4de3-9bd3-79e44bae7cf0 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df",
              "summary": "{\n  \"name\": \"df\",\n  \"rows\": 10,\n  \"fields\": [\n    {\n      \"column\": \"attention_mask\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"tensor([[1, 1, 1,  ..., 0, 0, 0]])\",\n          \"tensor([[1, 1, 1,  ..., 0, 0, 0]])\",\n          \"tensor([[1, 1, 1,  ..., 0, 0, 0]])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"input_ids\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"tensor([[25652,    25,  1867,  ..., 50256, 50256, 50256]])\",\n          \"tensor([[25652,    25,  1867,  ..., 50256, 50256, 50256]])\",\n          \"tensor([[25652,    25,  5338,  ..., 50256, 50256, 50256]])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"labels\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 10,\n        \"samples\": [\n          \"tensor([[25652,    25,  1867,  ..., 50256, 50256, 50256]])\",\n          \"tensor([[25652,    25,  1867,  ..., 50256, 50256, 50256]])\",\n          \"tensor([[25652,    25,  5338,  ..., 50256, 50256, 50256]])\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 63
        }
      ],
      "source": [
        "df = pd.DataFrame(test_data)\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class QADataset(Dataset):\n",
        "    def __init__(self, tokenized_data):\n",
        "        self.tokenized_data = tokenized_data\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.tokenized_data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = self.tokenized_data[idx]\n",
        "        return {\n",
        "            'input_ids': item['input_ids'].squeeze(),\n",
        "            'attention_mask': item['attention_mask'].squeeze(),\n",
        "            'labels': item['labels'].squeeze(),\n",
        "        }\n",
        "\n",
        "\n",
        "train_dataset = QADataset(train_data)\n",
        "test_dataset = QADataset(test_data)\n"
      ],
      "metadata": {
        "id": "3YVC4XyfYtoQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bZuj7_BRilJ1"
      },
      "outputs": [],
      "source": [
        "import evaluate\n",
        "\n",
        "metric = evaluate.load(\"accuracy\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "   logits, labels = eval_pred\n",
        "   predictions = np.argmax(logits, axis=-1)\n",
        "   return metric.compute(predictions=predictions, references=labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "BnXaWkkzSwWb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "outputId": "1b5ec5f4-da2e-45fe-88a3-76b4d220ea82"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: '_OpenCVImportHook' object has no attribute 'find_spec'",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0d80596a93f9>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtransformers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTrainingArguments\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGPT2LMHeadModel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"gpt2\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m training_args = TrainingArguments(\n\u001b[1;32m      5\u001b[0m     \u001b[0moutput_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"./gpt2_finetuned_qa\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_handle_fromlist\u001b[0;34m(module, fromlist, import_, recursive)\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1378\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1380\u001b[0;31m             \u001b[0mmodule\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_class_to_module\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1381\u001b[0m             \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodule\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1382\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/utils/import_utils.py\u001b[0m in \u001b[0;36m_get_module\u001b[0;34m(self, module_name)\u001b[0m\n\u001b[1;32m   1388\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_get_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1389\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mimportlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimport_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\".\"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodule_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1391\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1392\u001b[0m             raise RuntimeError(\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/__init__.py\u001b[0m in \u001b[0;36mimport_module\u001b[0;34m(name, package)\u001b[0m\n\u001b[1;32m    124\u001b[0m                 \u001b[0;32mbreak\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0mlevel\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_bootstrap\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gcd_import\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpackage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__version__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mconfiguration_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mPretrainedConfig\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collator\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataCollator\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataCollatorWithPadding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_data_collator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mdebug_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDebugOption\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDebugUnderflowOverflow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mhyperparameter_search\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mALL_HYPERPARAMETER_SEARCH_BACKENDS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdefault_hp_search_backend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglue_compute_metrics\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxnli_compute_metrics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m from .processors import (\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mDataProcessor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mInputExample\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/processors/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# limitations under the License.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mglue\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mglue_convert_examples_to_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglue_output_modes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglue_processors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglue_tasks_num_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0msquad\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSquadExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSquadFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSquadV1Processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSquadV2Processor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msquad_convert_examples_to_features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataProcessor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputExample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mInputFeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSingleSentenceClassificationProcessor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/data/processors/glue.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mis_tf_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_logger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdebugging\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdtypes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mexperimental\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec\u001b[0;34m(name, path, target)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.10/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_spec_legacy\u001b[0;34m(finder, name, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# fine tuned it without comute metrics\n",
        "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned_qa\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-4  # Example: Increase the learning rate\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "wz9Z8BoHY0e-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# fine tuned it with compute metrics\n",
        "\n",
        "from transformers import Trainer, TrainingArguments, GPT2LMHeadModel\n",
        "from datasets import load_dataset, load_metric\n",
        "import numpy as np\n",
        "\n",
        "# Load your dataset and metric\n",
        "metric = load_metric(\"accuracy\")\n",
        "\n",
        "model = GPT2LMHeadModel.from_pretrained(\"gpt2\")\n",
        "\n",
        "from datasets import load_metric\n",
        "import numpy as np\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=-1)\n",
        "    metric = load_metric(\"accuracy\")\n",
        "    return metric.compute(predictions=predictions, references=labels)\n",
        "\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./gpt2_finetuned_qa\",\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=2,\n",
        "    logging_dir='./logs',\n",
        "    logging_steps=10,\n",
        "    learning_rate=5e-4  # Example: Increase the learning rate\n",
        ")\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=test_dataset,\n",
        "    compute_metrics=compute_metrics  # Pass the function reference here\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 771
        },
        "id": "43vV0YuKkaIS",
        "outputId": "aadeed25-f9af-4c3b-d9b4-4422aa2ee848"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/datasets/load.py:756: FutureWarning: The repository for accuracy contains custom code which must be executed to correctly load the metric. You can inspect the repository content at https://raw.githubusercontent.com/huggingface/datasets/2.18.0/metrics/accuracy/accuracy.py\n",
            "You can avoid this message in future by passing the argument `trust_remote_code=True`.\n",
            "Passing `trust_remote_code=True` will be mandatory to load this metric from the next major release of `datasets`.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:432: FutureWarning: Passing the following arguments to `Accelerator` is deprecated and will be removed in version 1.0 of Accelerate: dict_keys(['dispatch_batches', 'split_batches', 'even_batches', 'use_seedable_sampler']). Please pass an `accelerate.DataLoaderConfiguration` instead: \n",
            "dataloader_config = DataLoaderConfiguration(dispatch_batches=None, split_batches=False, even_batches=True, use_seedable_sampler=True)\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='168' max='168' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [168/168 01:59, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>10</td>\n",
              "      <td>1.661500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>20</td>\n",
              "      <td>0.232200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>30</td>\n",
              "      <td>0.226100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>40</td>\n",
              "      <td>0.175700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>50</td>\n",
              "      <td>0.187300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>60</td>\n",
              "      <td>0.145600</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>70</td>\n",
              "      <td>0.093300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>80</td>\n",
              "      <td>0.071200</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>90</td>\n",
              "      <td>0.094300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.079100</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>110</td>\n",
              "      <td>0.081700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>120</td>\n",
              "      <td>0.042300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>130</td>\n",
              "      <td>0.042400</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>140</td>\n",
              "      <td>0.043300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>150</td>\n",
              "      <td>0.029800</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>160</td>\n",
              "      <td>0.036500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=168, training_loss=0.1945010315449465, metrics={'train_runtime': 120.458, 'train_samples_per_second': 2.764, 'train_steps_per_second': 1.395, 'total_flos': 174020493312000.0, 'train_loss': 0.1945010315449465, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ND5Uwx6q6ECB"
      },
      "outputs": [],
      "source": [
        "model.save_pretrained(\"/content/test_trainer\")\n",
        "tokenizer.save_pretrained(\"/content/test_trainer\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2KAoU0ukd-oa"
      },
      "outputs": [],
      "source": [
        "# connect to google drive\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dUrtzxKmjQjF"
      },
      "outputs": [],
      "source": [
        "#load any model from google drive\n",
        "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
        "\n",
        "model_path = '/content/drive/MyDrive/Colab Notebooks/Models/FirstLLM'\n",
        "\n",
        "# Load the language model\n",
        "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
        "\n",
        "# Load the tokenizer\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JOJh962AfM-8"
      },
      "outputs": [],
      "source": [
        "# save the model to google drive\n",
        "model.save_pretrained('/content/drive/MyDrive/Colab Notebooks/Models/FirstLLM')  # Replace with your desired path\n",
        "tokenizer.save_pretrained('/content/drive/MyDrive/Colab Notebooks/Models/FirstLLM')  # Replace with your desired path\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel ,GPT2Tokenizer\n",
        "model = GPT2LMHeadModel.from_pretrained(\"/content/test_trainer\")\n",
        "tokenizer = GPT2Tokenizer.from_pretrained(\"/content/test_trainer\")\n",
        "tokenizer.padding_side = \"right\"\n"
      ],
      "metadata": {
        "id": "MyIJ-8ADPwvL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h4IRhsGugJ1z"
      },
      "outputs": [],
      "source": [
        "# Example settings\n",
        "input_text = \"What is the purpose of edge detection in image processing?\"\n",
        "temperature = 0.5 # higher values lead to more creative, less predictable text\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel,AutoTokenizer\n",
        "\n",
        "import torch\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "      # Move the model to the chosen device\n",
        "\n",
        "\n",
        "# Encode input text and create attention mask\n",
        "input_ids = tokenizer.encode(input_text, return_tensors='pt') .to(device) # Move input_ids to the same device as the model\n",
        "attention_mask = torch.ones(input_ids.shape, dtype=torch.long)  # Ensure attention_mask is on the same device\n",
        "model.to(device)\n",
        "# Set the do_sample parameter to True for using temperature\n",
        "output = model.generate(\n",
        "    input_ids=input_ids,\n",
        "    attention_mask=attention_mask,\n",
        "    #max_length=200,\n",
        "    max_new_tokens=30,\n",
        "    temperature=temperature,\n",
        "    do_sample=True,  # Enable sampling\n",
        "    num_return_sequences=1,\n",
        "    pad_token_id=tokenizer.eos_token_id,\n",
        ")\n",
        "\n",
        "# Decode generated text\n",
        "generated_text = tokenizer.decode(output[0], skip_special_tokens=True,pad_token=\"right\")\n",
        "print(generated_text)"
      ],
      "metadata": {
        "id": "kCjkufanCQV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_answer(question, temperature=1):\n",
        "    # Tokenize the question with a prompt indicating that only an answer is expected\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "      # Move the model to the chosen device\n",
        "    model.to(device)\n",
        "    prompt = f\"question: {question} \\nanswer:\"\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "\n",
        "    # Generate a response with a specified temperature\n",
        "    output = model.generate(input_ids, max_length=50, eos_token_id=tokenizer.eos_token_id, temperature=temperature)\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # Extract only the answer part\n",
        "    answer_start = response.find(\"answer: \") + len(\"answer: \")\n",
        "    answer = response[answer_start:].strip()\n",
        "\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"What is the purpose of edge detection in image processing?\"\n",
        "answer = generate_answer(question, temperature=0.5)\n",
        "print(answer)\n"
      ],
      "metadata": {
        "id": "ujFxP2WVgwL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_qa_summary(topic, temperature=1.0):\n",
        "    # Check if GPU is available and set the device accordingly\n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "    # Move the model to the chosen device\n",
        "    model.to(device)\n",
        "\n",
        "    # Adjust the prompt to be more directive in generating QA pairs\n",
        "    prompt = f\"Create  quiz with question and answer about {topic}:\\n\\n\"\n",
        "\n",
        "    # Tokenize the prompt and move tensors to the same device as the model\n",
        "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
        "\n",
        "    # Adjust generation settings for more dynamic output\n",
        "    output = model.generate(\n",
        "        max_length=1024,\n",
        "        num_return_sequences=1,\n",
        "        eos_token_id=tokenizer.eos_token_id,\n",
        "        temperature=temperature,\n",
        "        no_repeat_ngram_size=3,\n",
        "        pad_token_id=tokenizer.eos_token_id,\n",
        "        top_k=50,\n",
        "        top_p=0.95,\n",
        "        do_sample=True\n",
        "\n",
        "    )\n",
        "    response = tokenizer.decode(output[0], skip_special_tokens=True)\n",
        "\n",
        "    # The response should contain a series of questions and answers\n",
        "    return response\n",
        "\n",
        "# Example usage\n",
        "topic = \" image processing\"\n",
        "\n",
        "print(\"summarization quiz with questions and answers about {topic}:\\n\\n\")\n",
        "for i in range(10):\n",
        "    qa_summary = generate_qa_summary(topic, temperature=0.7)\n",
        "    print(f'Question {i+1} {qa_summary}')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuBpS2X-umps",
        "outputId": "b72c127f-576e-4ec7-ab55-0cdcd3600eb4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "summarization quiz with questions and answers about {topic}:\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1 : What is the purpose of the Electronic Industry Citizenship Coalition (EICC)? context: Electronic Industry Citizens Coalition (EPICC): An organization promoting a common code of conduct for the electronics and ICT industry. Electronic Industry Assessment Tool (EAT): A system evaluating electronic products based on environmental criteria. answer: Evaluates electronic productsbased on environmental principles\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 2 : What is the goal in creating a linear filter in the context of morphological operations? context: Finite Differences as Convolutions: Forward difference (right slope), Backward difference (left slope), Central Difference (average slope). Definition: Function Gradient: Take average of left and right slope). answer: Take Average of left & right slope), backward difference\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 3 : What is the process of 'translation' in the context of morphological operations? context: Translation: A is set of pixels in binary image. B is a set of discrete function. x,y) is a 2D function. answer: 4D function\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 4 : What is the purpose of edge detection in image processing? context: Edge Detection Image processing task that finds edges and contours in images. Edges so important that human vision can reconstruct edge lines. answer: Finds edges and disables edge detection\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 5 : What is a successful application of a concept in morphological operations? context: Methods: Morphological Operations: 2 basic morphological filters, built from translations and reflections: Dilation and Erosion. Composite relations include Closing and Opening, Conditional Dilation. answer: Closing and opening, Condative Dilation\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 6 : What is the purpose of the dilation operator in morphological operations? context: Dilation operator is used in morphologic operations. Erosion operator is: Suppose A and B are sets of pixels, dilation of A by B. E.g., if we change B to dilation. answer: Dumping operator is called in morphologically operations\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 7 : What is the purpose of edge detection in image processing? context: Edge Detection Image processing task that finds edges and contours in images. Edges so important that human vision can reconstruct edge lines. answer: Finds edges and creours in image and compute edge lines\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 8 : What is the goal in creating an integrated linear filter in morphological operations? context: Finite Differences as Convolutions: Forward difference (right slope), Backward difference (left slope), Central Difference (average slope). Definition: Finites as Conventional Prewitt, Prewett, Prentice Hall. answer: Prewitting, Pending. answer? Forward difference, backward difference, Central Difference\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set `padding_side='left'` when initializing the tokenizer.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 9 : What is the concept of 'translation' in the context of morphological operations? context: Translation: A is set of pixels in binary image, B is an image. Note: An image is not digitally representable in the US. Translation: If we change B to dilation, we can reconstruct if B is a digitally representables. answer: Translation of A (black and white) is digitally representative of B, given the size of B and the type of B. answer is: The concept of applying a digitally representedable image\n",
            "Question 10 : What is the purpose of the Digital Image Processing Lecture 5 in Digital Image processing? context: Digital Image Filter: A digital image processing task that finds edges and contours in images. Edges so important that human vision can reconstruct edge lines. answer: Finds edges and lines\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "nXmwFMxKAONo"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "6749165758614836bf145f18ab612d52": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9fce8c5596c14f2f8ece65a7c430f6ff",
              "IPY_MODEL_3b2059a8fc2348708b8f2268de981596",
              "IPY_MODEL_8f333fc0bff34bcc8b1648a22c373bf0"
            ],
            "layout": "IPY_MODEL_242a1be3a5ff4e1390e98b54f567827a"
          }
        },
        "9fce8c5596c14f2f8ece65a7c430f6ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_03ef4d9cddba4c22a1398525465895b8",
            "placeholder": "​",
            "style": "IPY_MODEL_14bead6ae4904c7eaf75ada1297f5a90",
            "value": "Downloading data: 100%"
          }
        },
        "3b2059a8fc2348708b8f2268de981596": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d78699cbba7d43b696e9cca8e84d00ef",
            "max": 78263,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_3a13601d3d344f12b751e0ccc8e3ef85",
            "value": 78263
          }
        },
        "8f333fc0bff34bcc8b1648a22c373bf0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e237392c09104efa8ca4150d9f36f972",
            "placeholder": "​",
            "style": "IPY_MODEL_a9cb9efd8aa14b84a4c33a82eff4e7dc",
            "value": " 78.3k/78.3k [00:00&lt;00:00, 232kB/s]"
          }
        },
        "242a1be3a5ff4e1390e98b54f567827a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "03ef4d9cddba4c22a1398525465895b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "14bead6ae4904c7eaf75ada1297f5a90": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d78699cbba7d43b696e9cca8e84d00ef": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a13601d3d344f12b751e0ccc8e3ef85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e237392c09104efa8ca4150d9f36f972": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a9cb9efd8aa14b84a4c33a82eff4e7dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "143ab905681c4e0c993985885a748391": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_38f87f5b7966481db28c410adf780ec6",
              "IPY_MODEL_53bba74dad20454396c6d5ee5fbed06f",
              "IPY_MODEL_87cd41e8995245c9873e7879a4b353a7"
            ],
            "layout": "IPY_MODEL_afe78fca3d92478db1ef2b6b6798d2d1"
          }
        },
        "38f87f5b7966481db28c410adf780ec6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ab0091b4dff74b4595c5c0448188b8b0",
            "placeholder": "​",
            "style": "IPY_MODEL_0b86a28da9a1407397fc060f2486495a",
            "value": "Downloading data: 100%"
          }
        },
        "53bba74dad20454396c6d5ee5fbed06f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eacc944cf2645adb1e1db0993ea6294",
            "max": 6268,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8fbe8b5f2de540b18b0e89a0bf66b9b6",
            "value": 6268
          }
        },
        "87cd41e8995245c9873e7879a4b353a7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7f326ff878ed449fabd3dae56cfe3eee",
            "placeholder": "​",
            "style": "IPY_MODEL_0896619ec4034ff88b4d8bc4ccdbd117",
            "value": " 6.27k/6.27k [00:00&lt;00:00, 21.6kB/s]"
          }
        },
        "afe78fca3d92478db1ef2b6b6798d2d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ab0091b4dff74b4595c5c0448188b8b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b86a28da9a1407397fc060f2486495a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eacc944cf2645adb1e1db0993ea6294": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fbe8b5f2de540b18b0e89a0bf66b9b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7f326ff878ed449fabd3dae56cfe3eee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0896619ec4034ff88b4d8bc4ccdbd117": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9589e7c3dc274a38b437bb52cead0842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ed0bf3a5d1694735bd2a0b5620b7fbb3",
              "IPY_MODEL_11ac72cf9e32441e9d30877b3a6ee5a3",
              "IPY_MODEL_5c170b7245f743e0bfda9b542832006f"
            ],
            "layout": "IPY_MODEL_39aaa7267f5f49d7bc53d9a27eeb3585"
          }
        },
        "ed0bf3a5d1694735bd2a0b5620b7fbb3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2800fb313789421799729b6f5bb7de00",
            "placeholder": "​",
            "style": "IPY_MODEL_7de623ccda65466c93ddd3133bf0a68d",
            "value": "Generating train split: "
          }
        },
        "11ac72cf9e32441e9d30877b3a6ee5a3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_594b211ac9934c46bc6733ba30749afa",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_79e00c2dea6c469890bce0b19ed8560b",
            "value": 1
          }
        },
        "5c170b7245f743e0bfda9b542832006f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_81d1f1bb729c4a4fb1ee15e470417029",
            "placeholder": "​",
            "style": "IPY_MODEL_f9e1959105d04373b748a46807e4358c",
            "value": " 1/0 [00:00&lt;00:00, 12.97 examples/s]"
          }
        },
        "39aaa7267f5f49d7bc53d9a27eeb3585": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2800fb313789421799729b6f5bb7de00": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7de623ccda65466c93ddd3133bf0a68d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "594b211ac9934c46bc6733ba30749afa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "79e00c2dea6c469890bce0b19ed8560b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "81d1f1bb729c4a4fb1ee15e470417029": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f9e1959105d04373b748a46807e4358c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "135430c6a5d04147ab6bc45f7b8380b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_cfc487246dbf4bfeaac046ddec2d51d3",
              "IPY_MODEL_cbc92df1f95949a49d4343193c2ce253",
              "IPY_MODEL_54bdcf8101e64a8ab604050bce071242"
            ],
            "layout": "IPY_MODEL_8f160f2b775b43e6be4d42989e195afa"
          }
        },
        "cfc487246dbf4bfeaac046ddec2d51d3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_75cd40989fbc4ca38d5e7ceef7f0ad04",
            "placeholder": "​",
            "style": "IPY_MODEL_c126076c240c41fc9837b6c256b348de",
            "value": "Generating test split: "
          }
        },
        "cbc92df1f95949a49d4343193c2ce253": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b5309dddfbe64eeb9166c969d009887b",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f978e8b9eb894138bc8f5f46d1de7c4c",
            "value": 1
          }
        },
        "54bdcf8101e64a8ab604050bce071242": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d9f1a73e50994e4ab40f200a9df84b8a",
            "placeholder": "​",
            "style": "IPY_MODEL_ad2726c74f2f4de989710040fc81a3b7",
            "value": " 1/0 [00:00&lt;00:00, 24.87 examples/s]"
          }
        },
        "8f160f2b775b43e6be4d42989e195afa": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "75cd40989fbc4ca38d5e7ceef7f0ad04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c126076c240c41fc9837b6c256b348de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b5309dddfbe64eeb9166c969d009887b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "f978e8b9eb894138bc8f5f46d1de7c4c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "d9f1a73e50994e4ab40f200a9df84b8a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad2726c74f2f4de989710040fc81a3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}